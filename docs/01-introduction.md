# 데이터 분석 개요

## 왜 데이터 분석을 배워야 할까요?

현대 사회는 데이터의 시대입니다. 매일 생성되는 방대한 양의 데이터 속에서 의미 있는 인사이트를 찾아내는 능력은 이제 선택이 아닌 필수가 되었습니다. 이 워크샵에서는 AWS 클라우드 플랫폼을 활용하여 실제 현업에서 사용되는 데이터 분석 파이프라인을 구축하고 운영하는 방법을 학습합니다.

## 데이터 분석의 실무 활용 분야

### 🏦 금융 (Finance)
- **신용 평가**: 고객의 대출 상환 능력을 예측하여 리스크 관리
- **알고리즘 트레이딩**: 시장 데이터를 실시간으로 분석하여 자동 매매 결정
- **사기 탐지**: 이상 거래 패턴을 감지하여 금융 사기 방지
- **포트폴리오 최적화**: 과거 수익률 데이터를 바탕으로 최적의 투자 조합 구성

### 📈 마케팅 (Marketing)
- **고객 세분화**: 구매 패턴과 행동 데이터로 타겟 고객군 식별
- **개인화 추천**: 사용자의 과거 행동을 분석하여 맞춤형 상품 추천
- **캠페인 효과 측정**: A/B 테스트를 통한 마케팅 전략의 성과 분석
- **고객 생애 가치 예측**: 고객별 장기적 수익성 예측 및 관리

### 🏥 헬스케어 (Healthcare)
- **질병 진단 지원**: 의료 영상과 검사 결과를 분석하여 진단 정확도 향상
- **신약 개발**: 임상 시험 데이터 분석을 통한 약물 효능 검증
- **환자 모니터링**: 웨어러블 기기 데이터로 실시간 건강 상태 추적
- **병원 운영 최적화**: 환자 흐름과 자원 배치 데이터로 효율성 개선

### 🎓 교육 (Education)
- **학습 성과 분석**: 학생들의 학습 패턴과 성취도 데이터 분석
- **개인화 학습**: 개별 학습자의 강점과 약점을 파악하여 맞춤형 커리큘럼 제공
- **교육 콘텐츠 최적화**: 학습 효과가 높은 교육 방법과 자료 식별
- **중도 탈락 예측**: 학습 데이터를 통한 중도 포기 위험 학생 조기 발견

### 🚗 기타 산업 분야
- **제조업**: 품질 관리, 예측 정비, 공급망 최적화
- **유통업**: 재고 관리, 수요 예측, 가격 최적화
- **교통**: 경로 최적화, 교통 흐름 분석, 자율주행 기술
- **엔터테인먼트**: 콘텐츠 추천, 시청률 예측, 사용자 참여도 분석

## 현업 데이터 분석가들이 겪는 주요 페인 포인트

### 1. 데이터 품질 문제 😤
- **문제**: 불완전하고 일관성 없는 데이터로 인한 분석 결과의 신뢰성 저하
- **현실**: "데이터 분석가의 80%의 시간은 데이터 정제에 소모된다"
- **예시**: 서로 다른 시스템에서 같은 고객을 다른 ID로 관리하는 경우

### 2. 데이터 사일로 (Data Silos) 🏢
- **문제**: 부서별로 분산된 데이터로 인한 통합 분석의 어려움
- **현실**: 마케팅팀, 영업팀, 고객서비스팀이 각각 다른 도구와 데이터베이스 사용
- **결과**: 고객에 대한 360도 뷰 확보 불가능

### 3. 확장성 문제 📊
- **문제**: 데이터 양이 증가할수록 기존 시스템의 성능 저하
- **현실**: 로컬 서버나 단일 데이터베이스로는 빅데이터 처리 한계
- **예시**: 일일 로그 데이터가 TB 단위로 증가하는 상황

### 4. 실시간 처리의 어려움 ⚡
- **문제**: 배치 처리 중심의 전통적 방식으로는 실시간 의사결정 지원 한계
- **현실**: 고객이 웹사이트를 떠나기 전에 개인화된 추천을 제공해야 하는 상황
- **결과**: 비즈니스 기회 손실

### 5. 기술적 복잡성 🔧
- **문제**: 다양한 도구와 기술 스택을 연결하고 관리하는 복잡성
- **현실**: 데이터 엔지니어, 분석가, 과학자 간의 협업 도구 불일치
- **예시**: Python으로 개발한 모델을 Java 기반 프로덕션 시스템에 배포하는 어려움

## ETL의 필요성과 중요성

### ETL이란?
**ETL**은 **Extract(추출)**, **Transform(변환)**, **Load(적재)**의 줄임말로, 데이터 분석의 핵심 프로세스입니다.

### 왜 ETL이 필요할까요?

#### 1. 데이터 통합 (Data Integration)
```
여러 소스 → 하나의 분석 환경
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│   CRM 시스템   │    │  웹 로그 데이터  │    │  소셜미디어   │
│   (고객정보)   │    │   (행동데이터)   │    │   (피드백)    │
└─────────────┘    └─────────────┘    └─────────────┘
        │                   │                   │
        └───────────────────┼───────────────────┘
                           │
                    ┌─────────────┐
                    │  통합 데이터   │
                    │   웨어하우스   │
                    └─────────────┘
```

#### 2. 데이터 품질 향상
- **정제**: 중복 제거, 오타 수정, 형식 통일
- **검증**: 데이터 무결성 확인, 이상값 탐지
- **표준화**: 일관된 형식과 단위로 변환

#### 3. 분석 효율성 증대
- **사전 처리**: 분석에 최적화된 형태로 데이터 구조화
- **성능 최적화**: 인덱싱, 파티셔닝을 통한 쿼리 속도 향상
- **접근성**: 분석가들이 쉽게 사용할 수 있는 형태로 제공

### ETL vs ELT: 현대적 접근법
```
전통적 ETL:
Source → Transform → Target

현대적 ELT:
Source → Target → Transform (클라우드에서)
```

클라우드 환경에서는 **ELT** 방식이 더 유연하고 확장 가능합니다.

## 현업 데이터 분석 도구 생태계

### 🔄 데이터 처리 엔진
#### Apache Spark
- **용도**: 대용량 데이터 분산 처리
- **장점**: 메모리 기반 처리로 빠른 성능
- **사용 사례**: 실시간 스트리밍, 머신러닝 파이프라인

#### Apache Flink
- **용도**: 실시간 스트림 처리
- **장점**: 낮은 지연시간, 정확한 이벤트 시간 처리
- **사용 사례**: 실시간 대시보드, 이벤트 기반 알림

### 🔀 워크플로우 관리
#### Apache Airflow
- **용도**: 데이터 파이프라인 스케줄링 및 모니터링
- **장점**: Python 기반의 유연한 워크플로우 정의
- **사용 사례**: 일일 배치 작업, 복잡한 데이터 파이프라인 관리

#### Prefect
- **용도**: 현대적인 워크플로우 오케스트레이션
- **장점**: 클라우드 네이티브, 사용자 친화적 UI
- **사용 사례**: MLOps, 데이터 품질 모니터링

### 🔧 데이터 변환 도구
#### dbt (data build tool)
- **용도**: SQL 기반 데이터 변환 및 모델링
- **장점**: 버전 관리, 테스트, 문서화 자동화
- **사용 사례**: 데이터 웨어하우스 모델링, 분석용 데이터 마트 구축

#### Great Expectations
- **용도**: 데이터 품질 검증 및 모니터링
- **장점**: 자동화된 데이터 프로파일링, 품질 리포트
- **사용 사례**: 데이터 파이프라인 품질 보장

### 📊 시각화 및 BI 도구
#### Tableau
- **용도**: 비즈니스 인텔리전스 및 데이터 시각화
- **장점**: 드래그 앤 드롭 인터페이스, 강력한 시각화 기능
- **사용 사례**: 경영진 대시보드, 셀프서비스 분석

#### Apache Superset
- **용도**: 오픈소스 BI 플랫폼
- **장점**: 무료, 확장 가능, 다양한 데이터 소스 지원
- **사용 사례**: 팀 내부 대시보드, 데이터 탐색

### 🗄️ 데이터 저장소
#### Snowflake
- **용도**: 클라우드 데이터 웨어하우스
- **장점**: 자동 확장, 컴퓨팅과 스토리지 분리
- **사용 사례**: 대규모 분석 워크로드, 데이터 공유

#### Apache Iceberg
- **용도**: 오픈 테이블 포맷
- **장점**: ACID 트랜잭션, 스키마 진화, 시간 여행
- **사용 사례**: 데이터 레이크 관리, 대용량 테이블 최적화

## AWS 데이터 분석 솔루션의 장점

### 🚀 완전 관리형 서비스 (Fully Managed)
- **장점**: 인프라 관리 부담 없이 분석에 집중
- **예시**: AWS Glue는 서버 프로비저닝, 패치, 백업을 자동으로 처리
- **비교**: 온프레미스 Hadoop 클러스터 관리 vs AWS EMR

### 💰 비용 효율성
#### 사용한 만큼만 지불 (Pay-as-you-go)
```
전통적 방식:
- 최대 용량 기준으로 서버 구매
- 사용률이 낮아도 고정 비용 발생

AWS 방식:
- 실제 사용한 리소스만큼만 과금
- 필요에 따라 자동 확장/축소
```

#### 비용 최적화 예시
- **S3**: 데이터 액세스 패턴에 따른 스토리지 클래스 자동 전환
- **Athena**: 스캔한 데이터 양에 대해서만 과금
- **Glue**: ETL 작업 실행 시간에 대해서만 과금

### ⚡ 확장성과 성능
#### 자동 확장 (Auto Scaling)
- **Glue**: 작업 크기에 따라 자동으로 리소스 할당
- **Athena**: 동시 쿼리 수에 관계없이 일관된 성능
- **S3**: 무제한 스토리지 용량

#### 성능 최적화
- **Columnar Storage**: Parquet 형식으로 쿼리 성능 향상
- **Partitioning**: 데이터 파티셔닝으로 스캔 범위 최소화
- **Caching**: 자주 사용되는 데이터의 자동 캐싱

### 🔗 서비스 간 완벽한 통합
```
데이터 파이프라인 예시:
S3 (원본 데이터) 
  ↓
Glue Crawler (스키마 발견)
  ↓
Glue Data Catalog (메타데이터 저장)
  ↓
Glue ETL (데이터 변환)
  ↓
S3 (처리된 데이터)
  ↓
Athena (SQL 분석)
  ↓
QuickSight (시각화)
```

### 🔒 보안과 거버넌스
#### 세밀한 권한 관리
- **IAM**: 사용자별, 리소스별 세밀한 권한 설정
- **Lake Formation**: 데이터 레이크 수준의 보안 관리
- **CloudTrail**: 모든 API 호출 로깅 및 감사

#### 데이터 보호
- **암호화**: 저장 시 및 전송 시 자동 암호화
- **VPC**: 네트워크 수준의 격리
- **Compliance**: SOC, PCI DSS, HIPAA 등 규정 준수

### 🌍 글로벌 가용성
- **다중 리전**: 전 세계 어디서나 낮은 지연시간으로 접근
- **재해 복구**: 자동 백업 및 복구 기능
- **고가용성**: 99.9% 이상의 서비스 가용성 보장

## 이 워크샵에서 배울 내용

### 실습 중심 학습
1. **실제 데이터셋 활용**: "AI Tool Usage by Students" 데이터로 실무 경험
2. **전체 파이프라인 구축**: 데이터 수집부터 시각화까지 end-to-end 경험
3. **문제 해결 능력**: 실제 발생할 수 있는 오류와 해결 방법 학습

### 현업 스킬 습득
- **클라우드 네이티브 사고방식**: 확장 가능하고 비용 효율적인 아키텍처 설계
- **데이터 엔지니어링 기초**: ETL 파이프라인 구축 및 최적화
- **SQL 분석 능력**: 복잡한 비즈니스 질문을 SQL로 해결하는 방법

### 미래 준비
- **산업 표준 도구**: 현업에서 실제로 사용되는 AWS 서비스 경험
- **포트폴리오 구축**: GitHub에 공개된 프로젝트로 취업 준비
- **지속적 학습**: 추가 학습 리소스와 심화 과정 안내

---

다음 단계에서는 AWS 계정 설정부터 시작하여 실제 데이터 분석 파이프라인을 구축해보겠습니다. 준비되셨나요? 🚀